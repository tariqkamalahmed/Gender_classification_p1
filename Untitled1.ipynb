{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_selection import SelectKBest, chi2\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import csv                             \n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/Shared/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path, Text=None):\n",
    "    with open(path, 'r',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for line in reader:\n",
    "            (Lines, Character, Gender) = parseReview(line)\n",
    "            rawData.append((Lines,Character, Gender))\n",
    "\n",
    "def splitData(percentage):     # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Lines, _, Gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Lines)),Gender))\n",
    "    for (Lines, _, Gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Lines)),Gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReview(reviewLine):\n",
    "    \n",
    "    Lines = reviewLine[0]\n",
    "    Character = reviewLine[1]\n",
    "    Gender = reviewLine[2]\n",
    "    \n",
    "    return reviewLine[0], reviewLine[1], reviewLine[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "    \n",
    "# Text processing with Scikit-Learn, basics\n",
    "# Creating a vectorizer that can be used to extract a bag of words\n",
    "# representation from documents\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "\n",
    "\n",
    "    pos_tags = [x[1] for x in pos_tag(text)] \n",
    "    text = word_tokenize(text)\n",
    "    b = []\n",
    "    for word in text:\n",
    "        if word.isalpha(): # removing punctuation\n",
    "            if word not in stop_words: # removing stopwords or \"too common\" words\n",
    "                word = word.lower() # converting all letters to lower case \n",
    "#                 word = wordnet_lemmatizer.lemmatize(word)  # lemmatisation\n",
    "#                 word = stemmer.stem(word) # Using standard stemmer from the nltk\n",
    "                b.append(word)\n",
    "     \n",
    "    return b,pos_tags #returns both pos-tags and a list of words that have been pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "def toFeatureVector(put):\n",
    "    tokens,pos_tags = put\n",
    "# Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVector = {}\n",
    "    for token in tokens: #split words into tokens and pos\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1.0\n",
    "        else:\n",
    "            featureVector[token] = float(featureVector[token] + 1)\n",
    "            \n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1.0\n",
    "        else:\n",
    "            featureDict[token] = float(featureDict[token] + 1)\n",
    "\n",
    "        \n",
    "#     for pos in pos_tags: #split words into tokens and pos\n",
    "#         if pos not in featureVector:\n",
    "#             featureVector[pos] = 1.0\n",
    "#         else:\n",
    "#             featureVector[pos] = float(featureVector[pos] + 1)\n",
    "            \n",
    "#         if pos not in featureDict:\n",
    "#             featureDict[pos] = 1.0\n",
    "#         else:\n",
    "#             featureDict[pos] = float(featureDict[pos] + 1)    \n",
    "            \n",
    "    \n",
    "#     for i in range(1, len(tokens)):\n",
    "#             bigram = tokens[i-1] + \" \" + tokens[i]\n",
    "#             try:\n",
    "#                 featureVector[bigram] = 1 #+= 1.0/len(tokens)\n",
    "#             except KeyError:\n",
    "#                 featureVector[bigram] = 1 #= 1.0/len(tokens)\n",
    "#             try:\n",
    "#                 featureVector[bigram] += 1.0\n",
    "#             except KeyError:\n",
    "#                 featureVector[bigram] = 1.0\n",
    "                \n",
    "    \n",
    "#     sentence = len(tokens)\n",
    "#     featureDict['length']= sentence\n",
    "        \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.01, class_weight = \"balanced\"))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(text, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = [] # the filtered data from the dataset file        \n",
    "trainData = [] # the training data as a percentage of the total dataset \n",
    "testData = []  # the test data as a percentage of the total dataset        \n",
    "\n",
    "\n",
    "# references to the data files\n",
    "Path = 'training.csv'\n",
    "\n",
    "loadData(Path) \n",
    "\n",
    "splitData(0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Done training!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.57      0.59      0.58      1017\n",
      "        male       0.57      0.55      0.56      1006\n",
      "\n",
      "    accuracy                           0.57      2023\n",
      "   macro avg       0.57      0.57      0.57      2023\n",
      "weighted avg       0.57      0.57      0.57      2023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = trainClassifier(trainData)  # train the classifier\n",
    "testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "print(\"Done training!\")\n",
    "print(classification_report(testTrue, testPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # references to the data files\n",
    "# rawData = []         \n",
    "# trainData = []        \n",
    "# testData = []   \n",
    "\n",
    "# # references to the data files\n",
    "# Path = 'test.csv'\n",
    "# loadData(Path) \n",
    "\n",
    "\n",
    "# splitData(0)# used to split data between training and test setting to 0 means all values are in testdata\n",
    "\n",
    "# testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "# testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "# print(\"Done training!\")\n",
    "# print(classification_report(testTrue, testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
