{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_selection import SelectKBest, chi2\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import csv                             \n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/Shared/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path, Text=None):\n",
    "    with open(path, 'r',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for line in reader:\n",
    "            (Lines, Character, Gender) = parseReview(line)\n",
    "            rawData.append((Lines,Character, Gender))\n",
    "            \n",
    "\n",
    "\n",
    "def splitData(percentage):     # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Lines, _, Gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Lines)),Gender))\n",
    "    for (Lines, _, Gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Lines)),Gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReview(reviewLine):\n",
    "    \n",
    "    Lines = reviewLine[0]\n",
    "    Character = reviewLine[1]\n",
    "    Gender = reviewLine[2]\n",
    "    \n",
    "    return reviewLine[0], reviewLine[1], reviewLine[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "    \n",
    "# Text processing with Scikit-Learn, basics\n",
    "# Creating a vectorizer that can be used to extract a bag of words\n",
    "# representation from documents\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "# Input: a string of one review\n",
    "def preProcess(character_text):\n",
    "\n",
    "\n",
    "    pos_tags = [x[1] for x in pos_tag(character_text)] \n",
    "    text = word_tokenize(character_text)\n",
    "    b = []\n",
    "    for word in text:\n",
    "        if word.isalpha(): # removing punctuation\n",
    "            if word not in stop_words: # removing stopwords or \"too common\" words\n",
    "                word = word.lower() # converting all letters to lower case \n",
    "#                 word = wordnet_lemmatizer.lemmatize(word)  # lemmatisation\n",
    "#                 word = stemmer.stem(word) # Using standard stemmer from the nltk\n",
    "                b.append(word)\n",
    "     \n",
    "    return b,pos_tags #returns both pos-tags and a list of words that have been pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "def toFeatureVector(put):\n",
    "    tokens,pos_tags = put\n",
    "# Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVector = {}\n",
    "    for token in tokens: #split words into tokens and pos\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1.0\n",
    "        else:\n",
    "            featureVector[token] = float(featureVector[token] + 1)\n",
    "            \n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1.0\n",
    "        else:\n",
    "            featureVector[token] = float(featureDict[token] + 1)\n",
    "\n",
    "        \n",
    "#     for pos in pos_tags: #split words into tokens and pos\n",
    "#         if pos not in featureVector:\n",
    "#             featureVector[pos] = 1.0\n",
    "#         else:\n",
    "#             featureVector[pos] = float(featureVector[pos] + 1)\n",
    "            \n",
    "#         if pos not in featureDict:\n",
    "#             featureDict[pos] = 1.0\n",
    "#         else:\n",
    "#             featureDict[pos] = float(featureDict[pos] + 1)    \n",
    "            \n",
    "    \n",
    "#     for i in range(1, len(tokens)):\n",
    "#             bigram = tokens[i-1] + \" \" + tokens[i]\n",
    "#             try:\n",
    "#                 featureVector[bigram] = 1 #+= 1.0/len(tokens)\n",
    "#             except KeyError:\n",
    "#                 featureVector[bigram] = 1 #= 1.0/len(tokens)\n",
    "#             try:\n",
    "#                 featureVector[bigram] += 1.0\n",
    "#             except KeyError:\n",
    "#                 featureVector[bigram] = 1.0\n",
    "\n",
    "#     sentence = len(tokens)    \n",
    "#     featureDict['length']= sentence\n",
    "        \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results have been taken from the classification report on the training dataset using a classifier linear classifer with a parameter of C as 0.01\n",
    "\n",
    "#KEY\n",
    "[\n",
    "\n",
    "1=WORDS\n",
    "\n",
    "2=PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "3=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "4=LOWER+STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "5=LEMMA+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "6=STEM+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS\n",
    "]\n",
    "\n",
    "\n",
    "UNIGRAMS_1 = [\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.59      0.58      1017\n",
    "        male       0.57      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_2=[\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.58      0.57      1017\n",
    "        male       0.56      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_3=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.57      0.56      1017\n",
    "        male       0.55      0.54      0.54      1006\n",
    "\n",
    "    accuracy                           0.55      2023\n",
    "   macro avg       0.55      0.55      0.55      2023\n",
    "weighted avg       0.55      0.55      0.55      2023\n",
    "\n",
    "]\n",
    "\n",
    "UNIGRAMS_4=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.59      0.58      1017\n",
    "        male       0.57      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "\n",
    "UNIGRAMS_5=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.59      0.58      1017\n",
    "        male       0.57      0.54      0.55      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.56      0.56      2023\n",
    "weighted avg       0.57      0.57      0.56      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_6=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.58      0.57      1017\n",
    "        male       0.56      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.57      0.56      2023\n",
    "]\n",
    "\n",
    "conclusion: \n",
    "While training results were best with UNIGRAMS_4 and UNIGRAMS_1 preprocessing in use.\n",
    "Using UNIGRAMS_5 was found to be better than UNIGRAMS_6 while UNIGRAMS_1 was on par with UNIGRAMS_4 .\n",
    "UNIGRAMS_3 was found to be the worst performing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results have been taken from the classification report on the training dataset using a classifier linear classifer with a parameter of C as 0.01\n",
    "\n",
    "#KEY\n",
    "[\n",
    "\n",
    "1=WORDS\n",
    "\n",
    "2=PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "3=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "4=LOWER+STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "5=LEMMA+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "6=STEM+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS\n",
    "]\n",
    "\n",
    "POS_1=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.52      0.61      0.56      1017\n",
    "        male       0.52      0.43      0.47      1006\n",
    "\n",
    "    accuracy                           0.52      2023\n",
    "   macro avg       0.52      0.52      0.52      2023\n",
    "weighted avg       0.52      0.52      0.52      2023    \n",
    "]    \n",
    "POS_2=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.52      0.60      0.56      1017\n",
    "        male       0.52      0.44      0.48      1006\n",
    "\n",
    "    accuracy                           0.52      2023\n",
    "   macro avg       0.52      0.52      0.52      2023\n",
    "weighted avg       0.52      0.52      0.52      2023    \n",
    "]    \n",
    "POS_3=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.52      0.61      0.56      1017\n",
    "        male       0.52      0.44      0.48      1006\n",
    "\n",
    "    accuracy                           0.52      2023\n",
    "   macro avg       0.52      0.52      0.52      2023\n",
    "weighted avg       0.52      0.52      0.52      2023    \n",
    "]    \n",
    "POS_4=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.52      0.61      0.56      1017\n",
    "        male       0.52      0.44      0.48      1006\n",
    "\n",
    "    accuracy                           0.52      2023\n",
    "   macro avg       0.52      0.52      0.52      2023\n",
    "weighted avg       0.52      0.52      0.52      2023    \n",
    "]    \n",
    "POS_5=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.52      0.60      0.56      1017\n",
    "        male       0.53      0.45      0.48      1006\n",
    "\n",
    "    accuracy                           0.52      2023\n",
    "   macro avg       0.52      0.52      0.52      2023\n",
    "weighted avg       0.52      0.52      0.52      2023    \n",
    "]        \n",
    "POS_6=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.52      0.60      0.56      1017\n",
    "        male       0.52      0.44      0.47      1006\n",
    "\n",
    "    accuracy                           0.52      2023\n",
    "   macro avg       0.52      0.52      0.52      2023\n",
    "weighted avg       0.52      0.52      0.52      2023\n",
    "]\n",
    "\n",
    "conclusion: \n",
    "While training results were best with pos only was  pos_4 and pos_5\n",
    "However, using Pos taggers by itself was found to be worst than using just unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results have been taken from the classification report on the training dataset using a classifier linear classifer with a parameter of C as 0.01\n",
    "\n",
    "#KEY\n",
    "[\n",
    "\n",
    "1=WORDS\n",
    "\n",
    "2=PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "3=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "4=LOWER+STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "5=LEMMA+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "6=STEM+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS\n",
    "]\n",
    "\n",
    "UNIGRAMS_POS_1=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.59      0.58      1017\n",
    "        male       0.57      0.54      0.55      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.56      0.56      2023\n",
    "weighted avg       0.57      0.57      0.56      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_POS_2=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.58      0.57      1017\n",
    "        male       0.56      0.54      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "]\n",
    "\n",
    "\n",
    "UNIGRAMS_POS_3=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.57      0.56      1017\n",
    "        male       0.56      0.54      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_POS_4=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.58      0.57      1017\n",
    "        male       0.56      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_POS_5=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.57      0.56      1017\n",
    "        male       0.56      0.54      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "\n",
    "UNIGRAMS_POS_6=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.58      0.57      1017\n",
    "        male       0.56      0.54      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "\n",
    "]\n",
    "\n",
    "conclusion: \n",
    "While training results have unigrams+pos were best with UNIGRAMS_POS_4 preprocessing in use.\n",
    "Using UNIGRAMS_POS_6 was found to be better than UNIGRAMS_POS_5 while UNIGRAMS_POS_3 was equal.\n",
    "UNIGRAMS_POS_3 was found to be the worst performing.\n",
    "\n",
    "UNIGRAMS_POS_4 and UNIGRAMS_POS_4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While training results were best with UNIGRAMS_POS_4.\n",
    "Using UNIGRAMS_POS_6 was found to be better than UNIGRAMS_POS_5 while UNIGRAMS_POS_3 was equal.\n",
    "UNIGRAMS_POS_3 was found to be the worst performing.\n",
    "However results shows that using UNIGRAMS_POS_4 was worst performing than UNIGRAMS_4 on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*results have been taken from the classification report on the training dataset using a classifier linear classifer with a parameter of C as 0.01\n",
    "\n",
    "\n",
    "#KEY\n",
    "[\n",
    "\n",
    "1=WORDS\n",
    "\n",
    "2=PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "3=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "4=LOWER+STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "5=LEMMA+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS,\n",
    "\n",
    "6=STEM+LOWER=STOPWORDREM+PUNCTUATION_REMOVAL+WORDS\n",
    "]\n",
    "\n",
    "\n",
    "UNIGRAMS_BIGRAMS_POS_1=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.59      0.58      1017\n",
    "        male       0.57      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_POS_2=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.55      0.57      0.56      1017\n",
    "        male       0.55      0.53      0.54      1006\n",
    "\n",
    "    accuracy                           0.55      2023\n",
    "   macro avg       0.55      0.55      0.55      2023\n",
    "weighted avg       0.55      0.55      0.55      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_POS_3=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.55      0.57      0.56      1017\n",
    "        male       0.55      0.53      0.54      1006\n",
    "\n",
    "    accuracy                           0.55      2023\n",
    "   macro avg       0.55      0.55      0.55      2023\n",
    "weighted avg       0.55      0.55      0.55      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_POS_4=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.55      0.58      0.56      1017\n",
    "        male       0.55      0.52      0.54      1006\n",
    "\n",
    "    accuracy                           0.55      2023\n",
    "   macro avg       0.55      0.55      0.55      2023\n",
    "weighted avg       0.55      0.55      0.55      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_POS_5=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.59      0.57      1017\n",
    "        male       0.56      0.53      0.54      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_POS_6=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.59      0.57      1017\n",
    "        male       0.56      0.53      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_WORDS_6=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.60      0.59      1017\n",
    "        male       0.58      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "UNIGRAMS_BIGRAMS_WORDS_POS_SENTENCE_LENGTH_6=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.59      0.57      1017\n",
    "        male       0.56      0.53      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_WORDS_4=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.59      0.57      1017\n",
    "        male       0.56      0.53      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.1, class_weight = \"balanced\"))])\n",
    "    #pipeline =  Pipeline([('tfidf', TfidfTransformer()),('chi2', SelectKBest(chi2, k=20000)),('svc', LinearSVC(loss = 'hinge'))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIGRAMS_BIGRAMS_WORDS_STEM_POS_SENTENCE_LENGTH_TFIDFTRANSFORMER_K=1000=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.58      0.43      0.49      1017\n",
    "        male       0.54      0.69      0.61      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.55      2023\n",
    "weighted avg       0.56      0.56      0.55      2023\n",
    "]\n",
    "\n",
    "\n",
    "UNIGRAMS_TFIDFTRANSFORMER_K=1500=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.44      0.50      1017\n",
    "        male       0.54      0.66      0.59      1006\n",
    "\n",
    "    accuracy                           0.55      2023\n",
    "   macro avg       0.55      0.55      0.55      2023\n",
    "weighted avg       0.55      0.55      0.55      2023\n",
    "    \n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_WORDS_STEM_POS_SENTENCE_LENGTH_TFIDFTRANSFORMER_K=2000=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.58      0.45      0.51      1017\n",
    "        male       0.55      0.67      0.60      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.55      2023\n",
    "weighted avg       0.56      0.56      0.55      2023\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_WORDS_STEM_POS_SENTENCE_LENGTH_LINEARSVC_PARAMETER_SET_0.1=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.58      0.57      1017\n",
    "        male       0.56      0.55      0.55      1006\n",
    "\n",
    "    accuracy                           0.56      2023\n",
    "   macro avg       0.56      0.56      0.56      2023\n",
    "weighted avg       0.56      0.56      0.56      2023\n",
    "\n",
    "]\n",
    "\n",
    "UNIGRAMS_BIGRAMS_WORDS_STEM_POS_SENTENCE_LENGTH_LINEARSVC_PARAMETER_SET_0.01=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.57      0.59      0.58      1017\n",
    "        male       0.57      0.55      0.56      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "\n",
    "UNIGRAMS_BIGRAMS_WORDS_STEM_POS_SENTENCE_LENGTH_LINEARSVC_PARAMETER_SET_0.001=[\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      female       0.56      0.64      0.60      1017\n",
    "        male       0.58      0.50      0.54      1006\n",
    "\n",
    "    accuracy                           0.57      2023\n",
    "   macro avg       0.57      0.57      0.57      2023\n",
    "weighted avg       0.57      0.57      0.57      2023\n",
    "]\n",
    "\n",
    "conclusion: linearsvc classifer was used as with a parameter set to 0.1 as it showed the most similar results scores between males and females having the highest weight score and accuracy. It should be considered that the scores from the classifcation report were found with all the features and preprocessing techniques used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(text, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = [] # the filtered data from the dataset file        \n",
    "trainData = [] # the training data as a percentage of the total dataset \n",
    "testData = []  # the test data as a percentage of the total dataset        \n",
    "\n",
    "\n",
    "# references to the data files\n",
    "Path = 'training.csv'\n",
    "\n",
    "loadData(Path) \n",
    "\n",
    "splitData(0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Done training!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.56      0.58      0.57      1017\n",
      "        male       0.56      0.55      0.55      1006\n",
      "\n",
      "    accuracy                           0.56      2023\n",
      "   macro avg       0.56      0.56      0.56      2023\n",
      "weighted avg       0.56      0.56      0.56      2023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = trainClassifier(trainData)  # train the classifier\n",
    "testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "print(\"Done training!\")\n",
    "print(classification_report(testTrue, testPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.55      0.61      0.58       526\n",
      "        male       0.62      0.56      0.59       598\n",
      "\n",
      "    accuracy                           0.59      1124\n",
      "   macro avg       0.59      0.59      0.59      1124\n",
      "weighted avg       0.59      0.59      0.59      1124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# references to the data files\n",
    "rawData = []         \n",
    "trainData = []        \n",
    "testData = []   \n",
    "\n",
    "# references to the data files\n",
    "Path = 'test.csv'\n",
    "loadData(Path) \n",
    "\n",
    "\n",
    "splitData(0)# used to split data between training and test setting to 0 means all values are in testdata\n",
    "\n",
    "testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "print(\"Done training!\")\n",
    "print(classification_report(testTrue, testPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_heatmap(testTrue, testPred):\n",
    "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
    "    labels = list(set(testPred))   # get the labels in the y_test\n",
    "    # print(labels)\n",
    "    cm = confusion_matrix(testTrue, testPred, labels)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels( labels, rotation=0)\n",
    "    ax.set_yticklabels( labels)\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"orange\")\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_heatmap(testTrue, testPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the results we can not predict the gender with great reliability at 59% as it would mean that there is a large difference in the way that males and females speak within Eastenders.If we could be seen as the show being written poorly with defined sterotypes created in the way genders speak on the show.This can be seen by the colours indicted in the confusion matrix, the classifer was slightly better at a prediciting when a male was speaking than a female however it maybe due to more male lines were included in dataset than female lines leading to a high precision but a lower recall. However, we have seen that increasing the perprocessing techniques and features used does not always increase the classification scores. It was found that using unigrams with preprocessing techniques of using lowercases,stopwords removal and punctuation removal was best for the score on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
